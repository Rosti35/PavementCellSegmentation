{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FhdkMdRrtgba"
      },
      "source": [
        "# Import"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Pd9KSJZUsyG6"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.utils.data import DataLoader\n",
        "from torchvision import datasets, models, transforms\n",
        "from torchvision.models.segmentation import deeplabv3_resnet50\n",
        "import torch.optim as optim\n",
        "from torch.optim.lr_scheduler import StepLR\n",
        "from torchvision.transforms import functional as TF\n",
        "import os\n",
        "from PIL import Image\n",
        "import numpy as np\n",
        "from torch.utils.data import Dataset\n",
        "import torch\n",
        "from torchvision.transforms import v2\n",
        "\n",
        "from google.colab import drive\n",
        "\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "BASE_PATH = 'Path to dataset'"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0ub2XeYctkiw"
      },
      "source": [
        "# Dataset"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Tiles dataset for model training"
      ],
      "metadata": {
        "id": "t6E24sGaQfa_"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jXF9RXyFekZJ"
      },
      "outputs": [],
      "source": [
        "from PIL import Image\n",
        "import os\n",
        "from torch.utils.data import Dataset\n",
        "from torchvision.transforms import functional as TF\n",
        "import torch\n",
        "from torchvision.transforms import v2\n",
        "\n",
        "class RandomApplyTransforms:\n",
        "    def __init__(self, transforms):\n",
        "        self.transforms = transforms\n",
        "\n",
        "    def __call__(self, image, mask):\n",
        "        # Apply each transformation to both image and mask\n",
        "        for transform in self.transforms:\n",
        "            if torch.rand(1) < 0.5:\n",
        "                image = transform(image)\n",
        "                mask = transform(mask)\n",
        "        return image, mask\n",
        "\n",
        "custom_transform = RandomApplyTransforms([\n",
        "    TF.hflip,  # Horizontal flip\n",
        "    TF.vflip,  # Vertical flip\n",
        "\n",
        "])\n",
        "\n",
        "class TailsDatasetMask(Dataset):\n",
        "    def __init__(self, raw_data_dir, masks_dir, image_transform=None, mask_transform=None):\n",
        "        self.raw_data_dir = raw_data_dir\n",
        "        self.masks_dir = masks_dir\n",
        "        self.image_transform = image_transform\n",
        "        self.mask_transform = mask_transform\n",
        "        self.filenames = [f.split('.')[0] for f in os.listdir(masks_dir) if f.endswith('.png')]\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.filenames)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        # Load images\n",
        "        raw_image_path = os.path.join(self.raw_data_dir, self.filenames[idx] + '.png')\n",
        "        mask_path = os.path.join(self.masks_dir, self.filenames[idx] + '.png')\n",
        "\n",
        "        raw_image = Image.open(raw_image_path).convert('RGB')\n",
        "        mask = Image.open(mask_path).convert('L')\n",
        "\n",
        "        # Apply transformations\n",
        "        if self.image_transform is not None:\n",
        "            raw_image = self.image_transform(raw_image)\n",
        "\n",
        "\n",
        "        if self.mask_transform is not None:\n",
        "            mask = self.mask_transform(mask)\n",
        "\n",
        "        # mask = self.reverse_labels_in_mask(mask)\n",
        "        raw_image, mask = custom_transform(raw_image, mask)\n",
        "\n",
        "        return raw_image,  mask\n",
        "\n",
        "    def set_transform(self, raw_transform, mask_transform):\n",
        "\n",
        "      self.image_transform = raw_transform\n",
        "      self.mask_transform = mask_transform\n",
        "\n",
        "    def reverse_labels_in_mask(self, mask):\n",
        "        mask = 1 - mask\n",
        "        return mask\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XdedYD6Ye7pl"
      },
      "outputs": [],
      "source": [
        "from torch.utils.data import DataLoader, random_split\n",
        "\n",
        "raw_transform = v2.Compose([\n",
        "    v2.ColorJitter(brightness=0.5, contrast = 0.5),\n",
        "    v2.ToImage(), v2.ToDtype(torch.float32, scale=True)\n",
        "])\n",
        "\n",
        "test_raw_transform = v2.Compose([v2.ToImage(), v2.ToDtype(torch.float32, scale=True)])\n",
        "\n",
        "mask_transform = v2.Compose([v2.ToImage(), v2.ToDtype(torch.float32, scale=True)])\n",
        "\n",
        "\n",
        "\n",
        "# Initialize your dataset with the combined transform\n",
        "dataset_m = TailsDatasetMask(raw_data_dir=BASE_PATH+'Tiles/raw',\n",
        "                                  masks_dir=BASE_PATH+'Tiles/mask',\n",
        "                                  image_transform=raw_transform, mask_transform=mask_transform)\n",
        "\n",
        "# Split the dataset into training, validation, and test sets\n",
        "train_size = int(0.9 * len(dataset_m))\n",
        "val_size = int(0.1 * len(dataset_m))\n",
        "test_size = len(dataset_m) - train_size - val_size\n",
        "train_dataset, val_dataset, test_dataset = random_split(dataset_m, [train_size, val_size, test_size])\n",
        "\n",
        "\n",
        "# Create DataLoader objects for each dataset\n",
        "train_loader_mask = DataLoader(train_dataset, batch_size=30, shuffle=True, num_workers =6)\n",
        "val_loader_mask = DataLoader(val_dataset, batch_size=10, shuffle=False)\n",
        "test_loader_mask = DataLoader(test_dataset, batch_size=15, shuffle=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7zvnKPfIy22j"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.utils.data import DataLoader, random_split\n",
        "from torchvision.transforms import v2\n",
        "\n",
        "\n",
        "\n",
        "# Function to calculate class weights\n",
        "def calculate_class_weights(loader, num_classes=2):\n",
        "    class_counts = torch.zeros(num_classes)\n",
        "\n",
        "    for _, masks in loader:\n",
        "        masks = masks.view(-1)\n",
        "        class_counts[0] += (masks == 0).sum().item()\n",
        "        class_counts[1] += (masks == 1).sum().item()\n",
        "\n",
        "    # Calculate weights as the inverse of class frequency\n",
        "    total_count = class_counts.sum()\n",
        "    # print(class_counts )\n",
        "    class_weights = class_weights = total_count / (num_classes * class_counts)\n",
        "    return class_weights\n",
        "\n",
        "# Calculate weights for the training data\n",
        "class_weights = calculate_class_weights(train_loader_mask)\n",
        "print(f'Class Weights: {class_weights}')\n",
        "\n",
        "# Define the loss function with class weights\n",
        "criterion_seg = nn.BCEWithLogitsLoss(pos_weight=class_weights[1])\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Z4XkY9XwyP7i"
      },
      "source": [
        "# Model setup"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GgnRaYV-4dvD"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.utils.data import DataLoader\n",
        "from torchvision import datasets, models, transforms\n",
        "from torchvision.models.segmentation import deeplabv3_resnet50, fcn_resnet50, deeplabv3_mobilenet_v3_large, deeplabv3_resnet101, FCN_ResNet50_Weights, DeepLabV3_ResNet50_Weights\n",
        "import torch.optim as optim\n",
        "from torch.optim.lr_scheduler import StepLR\n",
        "from torchvision.transforms import functional as TF\n",
        "import os\n",
        "from PIL import Image\n",
        "import numpy as np"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def create_deeplabv3(outputchannels=1):\n",
        "    model = deeplabv3_resnet50(pretrained=True, progress=True)\n",
        "    # Change the classifier's output channels\n",
        "    model.classifier[4] = nn.Conv2d(256, outputchannels, kernel_size=(1, 1), stride=(1, 1))\n",
        "    model.aux_classifier[4] = nn.Conv2d(256, outputchannels, kernel_size=(1, 1), stride=(1, 1))\n",
        "    return model"
      ],
      "metadata": {
        "id": "Zkt-t_EpalYe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nzOsbzs_04Vc"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "\n",
        "# Define loss functions\n",
        "# Define the loss function with class weights\n",
        "criterion_seg = nn.BCEWithLogitsLoss()\n",
        "criterion_recon = nn.L1Loss()\n",
        "\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "model = create_deeplabv3().to(device)\n",
        "\n",
        "optimizer = optim.Adam(model.parameters(), lr=0.0001)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Train"
      ],
      "metadata": {
        "id": "UUdjwt9mUMJE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def train_other(model, dataloader, optimizer, device, train_recon=False, freeze_encoder=False):\n",
        "    model.train()\n",
        "    running_loss = 0.0\n",
        "    pbar = tqdm(enumerate(dataloader), total=len(dataloader), leave=False)\n",
        "    for batch_idx, (data, target) in pbar:\n",
        "        data = data.to(device)\n",
        "        target = target.to(device)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        if train_recon:\n",
        "            output = model(data)\n",
        "            loss = criterion_recon(output, target)\n",
        "        else:\n",
        "            output = model(data)['out']\n",
        "            loss = criterion_seg(output, target)\n",
        "\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        running_loss += loss.item()\n",
        "        pbar.set_description(f'Epoch Loss: {running_loss/(batch_idx+1):.4f}')\n",
        "\n",
        "    print(f'Training Loss: {running_loss / len(dataloader):.4f}')\n",
        "    return running_loss / len(dataloader)"
      ],
      "metadata": {
        "id": "vxiyE2unczl0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Test"
      ],
      "metadata": {
        "id": "ETE0UfhvUQDE"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xS0abNhf1MMZ"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "\n",
        "def accuracy(output, target):\n",
        "    preds = output.round()  # Round predictions to 0 or 1\n",
        "    correct = (preds == target).float()  # Element-wise equality\n",
        "    acc = correct.sum() / correct.numel()\n",
        "    return acc\n",
        "\n",
        "def test(model, dataloader, device, test_recon=False, num_visualizations=3):\n",
        "    model.eval()\n",
        "    running_loss = 0.0\n",
        "    total_accuracy = 0.0 if not test_recon else None\n",
        "    visualized = 0\n",
        "    with torch.no_grad(), tqdm(enumerate(dataloader), total=len(dataloader), leave=False) as pbar:\n",
        "        for batch_idx, (data, target) in pbar:\n",
        "            data = data.to(device)\n",
        "            if test_recon:\n",
        "                target = data\n",
        "            else:\n",
        "                target = target.to(device)\n",
        "\n",
        "            # output = model(data, train_recon=test_recon, freeze_encoder=False)\n",
        "            output = model(data)['out']\n",
        "\n",
        "            loss = criterion_recon(output, target) if test_recon else criterion_seg(output, target)\n",
        "            running_loss += loss.item()\n",
        "\n",
        "            if not test_recon:\n",
        "                acc = accuracy(output, target)\n",
        "                total_accuracy += acc.item()\n",
        "\n",
        "            # Visualization for the first N samples\n",
        "            if visualized < num_visualizations:\n",
        "                if test_recon:\n",
        "                    fig, ax = plt.subplots(1, 2, figsize=(12, 4))\n",
        "                    ax[0].imshow(data[0].cpu().permute(1, 2, 0))\n",
        "                    ax[0].set_title(\"Input Image\")\n",
        "                    ax[1].imshow(output[0].cpu().permute(1, 2, 0))\n",
        "                    ax[1].set_title(\"Reconstructed Image\")\n",
        "                    # ax[2].imshow(target[0].cpu().numpy().transpose(1, 2, 0))\n",
        "                    # ax[2].set_title(\"Target Image\")\n",
        "                    plt.show()\n",
        "                else:\n",
        "                    preds = torch.sigmoid(output[0].cpu().squeeze()) > 0.9\n",
        "                    # preds = output[0].cpu().squeeze()\n",
        "                    # Assuming segmentation masks are single-channel\n",
        "                    fig, ax = plt.subplots(1, 2, figsize=(12, 4))\n",
        "                    ax[0].imshow(data[0].cpu().numpy().transpose(1, 2, 0))\n",
        "                    ax[0].set_title(\"Input Image\")\n",
        "                    ax[1].imshow(preds, cmap='gray')\n",
        "                    ax[1].set_title(\"Predicted Mask\")\n",
        "                    # print(output[0].cpu().squeeze())\n",
        "                    # ax[2].imshow(target[0].cpu(), cmap='gray')\n",
        "                    # ax[2].set_title(\"True Mask\")\n",
        "                    # print(target[0].cpu().squeeze())\n",
        "                    plt.show()\n",
        "                visualized += 1\n",
        "\n",
        "\n",
        "            pbar.set_description(f'Batch {batch_idx+1}, Loss: {loss.item():.4f}')\n",
        "\n",
        "    epoch_loss = running_loss / len(dataloader)\n",
        "    print(f'Test Loss: {epoch_loss:.4f}')\n",
        "    if not test_recon:\n",
        "        epoch_accuracy = total_accuracy / len(dataloader)\n",
        "        print(f'Test Accuracy: {epoch_accuracy:.4f}')\n",
        "\n",
        "    return epoch_loss\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1fQscIdARUlN"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Validate"
      ],
      "metadata": {
        "id": "ScdoMDEbUVGW"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dbtspfTJTBtf"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import DataLoader\n",
        "from torchvision import transforms\n",
        "from tqdm import tqdm\n",
        "import torch\n",
        "from skimage.morphology import skeletonize\n",
        "\n",
        "def calculate_accuracy(output, target):\n",
        "    # Assuming output is logits, threshold to get binary prediction\n",
        "    preds = torch.sigmoid(output) > 0.9\n",
        "\n",
        "    correct = (preds == target).float()  # Convert to float for division\n",
        "\n",
        "    accuracy = correct.sum() / correct.numel()\n",
        "    return accuracy\n",
        "\n",
        "def calculate_precision_recall_f1(output, target, epsilon=1e-7):\n",
        "    preds = torch.sigmoid(output) > 0.9\n",
        "    true_positives = (preds * target).sum()\n",
        "    predicted_positives = preds.sum()\n",
        "    actual_positives = target.sum()\n",
        "\n",
        "    precision = true_positives / (predicted_positives + epsilon)\n",
        "    recall = true_positives / (actual_positives + epsilon)\n",
        "    f1 = 2 * (precision * recall) / (precision + recall + epsilon)\n",
        "    return precision, recall, f1\n",
        "\n",
        "def calculate_iou(output, target, epsilon=1e-7):\n",
        "    preds = torch.sigmoid(output) > 0.9\n",
        "    # skeleton = skeletonize(preds)\n",
        "    intersection = (preds * target).sum()\n",
        "    union = preds.sum() + target.sum() - intersection\n",
        "    iou = intersection / (union + epsilon)\n",
        "    return iou"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hCTwrEY0Rd5A"
      },
      "outputs": [],
      "source": [
        "def validation(model, device, val_loader, criterion):\n",
        "    model.eval()\n",
        "    val_loss = 0.0\n",
        "    total_accuracy = 0.0\n",
        "    total_precision = 0.0\n",
        "    total_recall = 0.0\n",
        "    total_f1 = 0.0\n",
        "    total_iou = 0.0\n",
        "    with torch.no_grad():\n",
        "        for data, target in val_loader:\n",
        "            data, target = data.to(device), target.to(device)\n",
        "\n",
        "            # output = model(data)\n",
        "            output = model(data)['out']\n",
        "\n",
        "            val_loss += criterion(output, target).item()\n",
        "\n",
        "            accuracy = calculate_accuracy(output, target)\n",
        "            precision, recall, f1 = calculate_precision_recall_f1(output, target)\n",
        "            iou = calculate_iou(output, target)\n",
        "\n",
        "            total_accuracy += accuracy.item()\n",
        "            total_precision += precision.item()\n",
        "            total_recall += recall.item()\n",
        "            total_f1 += f1.item()\n",
        "            total_iou += iou.item()\n",
        "\n",
        "    # Calculate averages\n",
        "    num_batches = len(val_loader)\n",
        "    avg_loss = val_loss / num_batches\n",
        "    avg_accuracy = total_accuracy / num_batches\n",
        "    avg_precision = total_precision / num_batches\n",
        "    avg_recall = total_recall / num_batches\n",
        "    avg_f1 = total_f1 / num_batches\n",
        "    avg_iou = total_iou / num_batches\n",
        "\n",
        "\n",
        "    avg_metrics = {\n",
        "    \"avg_val_loss\": avg_loss,\n",
        "    \"avg_val_accuracy\": avg_accuracy,\n",
        "    \"avg_val_precision\": avg_precision,\n",
        "    \"avg_val_recall\": avg_recall,\n",
        "    \"avg_val_f1\": avg_f1,\n",
        "    \"avg_val_iou\": avg_iou,\n",
        "    }\n",
        "\n",
        "    # wandb.log(avg_metrics)\n",
        "    print(f'Validation Loss: {avg_loss:.4f}, Accuracy: {avg_accuracy:.4f}, Precision: {avg_precision:.4f}, Recall: {avg_recall:.4f}, F1: {avg_f1:.4f}, IoU: {avg_iou:.4f}')\n",
        "\n",
        "    return avg_loss, avg_accuracy, avg_iou, avg_f1"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jwSQTd_tU8GD"
      },
      "source": [
        "# Train the model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jcqQV-BeRMJL"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import gc\n",
        "\n",
        "def cleanup_gpu():\n",
        "    gc.collect()\n",
        "    torch.cuda.empty_cache()\n",
        "\n",
        "cleanup_gpu()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QHzevFUPl4w5"
      },
      "outputs": [],
      "source": [
        "from IPython.display import clear_output\n",
        "\n",
        "# Function to plot the training and validation losses in real-time\n",
        "def plot_losses(epoch_losses, val_losses):\n",
        "    clear_output(wait=True)\n",
        "    plt.figure(figsize=(10, 5))\n",
        "    plt.plot(epoch_losses, label='Training Loss')\n",
        "    plt.plot(val_losses, label='Validation Loss')\n",
        "    plt.xlabel('Epochs')\n",
        "    plt.ylabel('Loss')\n",
        "    plt.title('Training and Validation Loss over Epochs')\n",
        "    plt.legend()\n",
        "    plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4-zDU0AEnzNv"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "\n",
        "\n",
        "# Initialize early stopping parameters\n",
        "patience = 5  # Number of epochs to wait for improvement before stopping\n",
        "best_val_loss = float('inf')\n",
        "patience_counter = 0\n",
        "train_losess = []\n",
        "val_losess = []\n",
        "\n",
        "num_epochs = 100  # Adjust as needed\n",
        "for epoch in range(1, num_epochs + 1):\n",
        "    print(f\"Epoch {epoch}/{num_epochs}\")\n",
        "\n",
        "    # train(model, train_loader_rec, optimizer, device, train_recon=True, freeze_encoder=False)\n",
        "\n",
        "    train_loss=train(model, train_loader_mask, optimizer, device, train_recon=False, freeze_encoder=False)\n",
        "\n",
        "    #train_loss = train_combined(model, train_loader_mask, train_loader_rec, optimizer, device, lambda_recon=1)\n",
        "\n",
        "    val_loss, avg_accuracy, avg_iou, avg_f1 = validation(model, device, val_loader_mask, criterion_seg)\n",
        "\n",
        "    #val_loss = test(model, val_loader_rec, device, test_recon=True)\n",
        "    print(val_loss)\n",
        "    test(model, val_loader_mask, device, test_recon=False)\n",
        "\n",
        "    train_losess.append(train_loss)\n",
        "    val_losess.append(val_loss)\n",
        "    plot_losses(train_losess, val_losess)\n",
        "\n",
        "    # Check for improvement\n",
        "    if val_loss < best_val_loss:\n",
        "        best_val_loss = val_loss\n",
        "        patience_counter = 0  # Reset patience counter if improvement is seen\n",
        "        # Save the best model\n",
        "        torch.save(model.state_dict(), 'best_model.pt')\n",
        "    else:\n",
        "        patience_counter += 1  # Increment patience counter if no improvement\n",
        "        print(patience_counter)\n",
        "\n",
        "    print(f'Validation Loss: {val_loss:.4f}, Accuracy: {avg_accuracy:.4f}, IoU: {avg_iou:.4f}, F1: {avg_f1:.4f}')\n",
        "\n",
        "    # Check if patience is exceeded\n",
        "    if patience_counter >= patience:\n",
        "        print(f'Early stopping triggered. No improvement in validation loss for {patience} consecutive epochs.')\n",
        "        break\n",
        "\n",
        "# Load the best model after training\n",
        "model.load_state_dict(torch.load('best_model.pt'))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Evaluate the model"
      ],
      "metadata": {
        "id": "Bl3LcpRNS1Tf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Load the best model after training\n",
        "model.load_state_dict(torch.load('best_model.pt'))"
      ],
      "metadata": {
        "id": "BGwS1kaAUiw-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NdOVhSbgD8ZV"
      },
      "outputs": [],
      "source": [
        "validation(model, device, val_loader_mask, criterion_seg)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "j849YqPVsdQJ"
      },
      "outputs": [],
      "source": [
        "# For testing segmentation\n",
        "test(model, val_loader_mask, device, test_recon=False)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Save the model"
      ],
      "metadata": {
        "id": "0nstxrl1SHb2"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "k6RrsLJjQgcX"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "\n",
        "def save_checkpoint(model, optimizer, epoch, filename):\n",
        "    checkpoint = {\n",
        "        'epoch': epoch,\n",
        "        'model_state_dict': model.state_dict(),\n",
        "        'optimizer_state_dict': optimizer.state_dict(),\n",
        "    }\n",
        "    torch.save(checkpoint, BASE_PATH+\"/models/\"+filename)\n",
        "    print(f\"Checkpoint saved to {filename}\")\n",
        "\n",
        "\n",
        "def load_checkpoint(model, optimizer, filename):\n",
        "    checkpoint = torch.load(BASE_PATH+\"/models/\"+filename)\n",
        "    model.load_state_dict(checkpoint['model_state_dict'])\n",
        "    optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
        "    epoch = checkpoint['epoch']\n",
        "    print(f\"Model and optimizer loaded from checkpoint at epoch {epoch}\")\n",
        "    return epoch"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DqAdMxee34np"
      },
      "source": [
        "# Predict the full image"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XTy6XfgWr-V_"
      },
      "outputs": [],
      "source": [
        "\n",
        "import torch\n",
        "import numpy as np\n",
        "from PIL import Image\n",
        "from torchvision.transforms import functional as TF\n",
        "from skimage.morphology import skeletonize\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "def predict_mask_full_image(model, image, tile_size=256, overlap=64, device='cuda'):\n",
        "    model.eval()#\n",
        "\n",
        "    # Calculate necessary padding to make the image divisible into tiles\n",
        "    width, height = image.size\n",
        "    pad_height = (tile_size - (height % (tile_size - overlap))) % (tile_size - overlap)\n",
        "    pad_width = (tile_size - (width % (tile_size - overlap))) % (tile_size - overlap)\n",
        "    print(f\"Original image size: {width}x{height}, Padding: {pad_width}x{pad_height}\")\n",
        "\n",
        "    # Pad image\n",
        "    padded_image = TF.pad(image, padding=(0, 0, pad_width, pad_height), padding_mode='reflect')\n",
        "    print(f\"Padded image size: {padded_image.width}x{padded_image.height}\")\n",
        "\n",
        "    full_mask = np.zeros((padded_image.height, padded_image.width))\n",
        "    count_map = np.zeros((padded_image.height, padded_image.width))\n",
        "\n",
        "\n",
        "    plt.imshow(padded_image)\n",
        "    plt.title(\"Padded Image\")\n",
        "    plt.show()\n",
        "\n",
        "    tile_count = 0\n",
        "\n",
        "    # Generate and process tiles\n",
        "    for y in range(0, padded_image.height - overlap, tile_size - overlap):\n",
        "        for x in range(0, padded_image.width - overlap, tile_size - overlap):\n",
        "            tile_count += 1\n",
        "\n",
        "            tile = padded_image.crop((x, y, min(x + tile_size, padded_image.width), min(y + tile_size, padded_image.height)))\n",
        "            tile_padded = TF.pad(tile, padding=(0, 0, tile_size - tile.width, tile_size - tile.height))  # Pad tile to ensure 256x256\n",
        "\n",
        "            # Process tile\n",
        "            tile_padded_tensor = TF.to_tensor(tile_padded).unsqueeze(0).to(device)  # Convert to tensor and add batch dimension\n",
        "\n",
        "            # Predict mask for tile\n",
        "            with torch.no_grad():\n",
        "                tile_mask_tensor = model(tile_padded_tensor)[\"out\"].squeeze().cpu()\n",
        "                tile_mask = torch.sigmoid(tile_mask_tensor).numpy() > 0.9\n",
        "                # tile_mask = skeletonize(tile_mask)\n",
        "                tile_mask = 1 - tile_mask\n",
        "            # Resize mask back to original tile size if padding was added\n",
        "            tile_mask_resized = tile_mask[:tile.height, :tile.width]\n",
        "\n",
        "\n",
        "\n",
        "            # Update full mask and count map\n",
        "            full_mask[y:y + tile.height, x:x + tile.width] += tile_mask_resized\n",
        "            count_map[y:y + tile.height, x:x + tile.width] += 1\n",
        "\n",
        "    # Average the overlaps\n",
        "    full_mask /= count_map\n",
        "\n",
        "    # Crop out any extra padding added to the image\n",
        "    final_mask = full_mask[:height, :width]\n",
        "\n",
        "\n",
        "    plt.imshow(final_mask, cmap='gray')\n",
        "    plt.title(\"Final Mask\")\n",
        "    plt.savefig(\"final.png\")\n",
        "    plt.show()\n",
        "\n",
        "    print(f\"Processed {tile_count} tiles.\")\n",
        "    return final_mask\n",
        "\n",
        "raw_data_dir= BASE_PATH+'Raw_data'\n",
        "filenames = [f.split('.')[0] for f in os.listdir(raw_data_dir) if f.endswith('.tif')]\n",
        "raw_image_path = os.path.join(raw_data_dir, filenames[60] + '.tif')\n",
        "raw_image = Image.open(raw_image_path).convert('RGB')\n",
        "\n",
        "final_mask = predict_mask_full_image(model, raw_image, device='cuda')\n",
        "Image.fromarray((final_mask * 255).astype(np.uint8)).save(raw_image.filename.split('.')[0] + '_mask.png')\n",
        "print(final_mask.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-JFOcfVeUMsL"
      },
      "outputs": [],
      "source": [
        "from PIL import Image, ImageEnhance, ImageChops\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "def overlay_mask_on_image(raw_image, mask):\n",
        "\n",
        "    if not isinstance(raw_image, Image.Image):\n",
        "        raise ValueError(\"raw_image must be a PIL.Image.Image object\")\n",
        "    if not isinstance(mask, Image.Image):\n",
        "        raise ValueError(\"mask must be a PIL.Image.Image object\")\n",
        "\n",
        "    # Resize mask to match the raw image size if necessary\n",
        "    if raw_image.size != mask.size:\n",
        "        mask = mask.resize(raw_image.size, Image.BILINEAR)\n",
        "\n",
        "    # Convert the mask to 'L' mode if it's not already\n",
        "    single_channel_image = mask.convert('L')\n",
        "    # mask = ImageEnhance.Contrast(mask).enhance(2.0)\n",
        "    #high_contrast_mask = Image.fromarray(((np_mask)*255).astype(np.uint8)).convert('L')\n",
        "    single_channel_image.save(\"high_contrast_mask.png\")\n",
        "\n",
        "\n",
        "    # Create an RGBA version of the single-channel image with some transparency\n",
        "    alpha = 10 # Adjust the alpha value to control transparency\n",
        "    single_channel_rgba = Image.merge('RGBA', (single_channel_image, single_channel_image, single_channel_image, Image.new('L', single_channel_image.size, alpha)))\n",
        "\n",
        "    # Composite the single-channel image onto the RGB image\n",
        "    composite_image = Image.alpha_composite(raw_image.convert('RGBA'), single_channel_rgba)\n",
        "\n",
        "\n",
        "    return composite_image\n",
        "\n",
        "Image.fromarray((final_mask * 255).astype(np.uint8)).save(\"final.png\")\n",
        "overlay_image = overlay_mask_on_image(raw_image, Image.fromarray((final_mask * 255).astype(np.uint8)))\n",
        "\n",
        "# Display the result\n",
        "plt.figure(figsize=(30, 30))\n",
        "plt.imshow(overlay_image)\n",
        "plt.axis('off')\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "G5i7Ry6p4tMH"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import cv2\n",
        "import numpy as np\n",
        "from skimage.morphology import skeletonize\n",
        "\n",
        "# Load the uploaded image\n",
        "image_path = 'high_contrast_mask-2.png'\n",
        "image = cv2.imread(image_path, cv2.IMREAD_GRAYSCALE)\n",
        "\n",
        "# Apply skeletonization using skimage\n",
        "binary_image = image > 200\n",
        "skeleton = skeletonize(binary_image)\n",
        "\n",
        "# Save the skeletonized image\n",
        "skeletonized_image_path = 'skeletonized_image.png'\n",
        "plt.imsave(skeletonized_image_path, skeleton, cmap='gray')\n",
        "\n",
        "# Display the original and skeletonized images\n",
        "fig, ax = plt.subplots(1, 2, figsize=(12, 6))\n",
        "ax[0].imshow(image, cmap='gray')\n",
        "ax[0].set_title('Original Image')\n",
        "ax[0].axis('off')\n",
        "\n",
        "ax[1].imshow(skeleton, cmap='gray')\n",
        "ax[1].set_title('Skeletonized Image')\n",
        "ax[1].axis('off')\n",
        "\n",
        "plt.show()\n",
        "\n",
        "skeletonized_image_path\n"
      ]
    }
  ]
}