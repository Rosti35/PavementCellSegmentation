{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FhdkMdRrtgba"
      },
      "source": [
        "# Import"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Pd9KSJZUsyG6"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.utils.data import DataLoader\n",
        "from torchvision import datasets, models, transforms\n",
        "from torchvision.models.segmentation import deeplabv3_resnet50\n",
        "import torch.optim as optim\n",
        "from torch.optim.lr_scheduler import StepLR\n",
        "from torchvision.transforms import functional as TF\n",
        "import os\n",
        "from PIL import Image\n",
        "import numpy as np\n",
        "from PIL import Image\n",
        "import os\n",
        "from torch.utils.data import Dataset\n",
        "import torch\n",
        "from torchvision.transforms import v2\n",
        "\n",
        "from google.colab import drive\n",
        "\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "BASE_PATH = 'Path to dataset'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5nJfA9H_tH3O"
      },
      "outputs": [],
      "source": [
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0ub2XeYctkiw"
      },
      "source": [
        "# Dataset"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DFeuZeL7erAb"
      },
      "source": [
        "## Tiles dataset for model training (reconstruction)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "d6Qer9mivFps"
      },
      "outputs": [],
      "source": [
        "\n",
        "\n",
        "class RandomApplyTransforms:\n",
        "    def __init__(self, transforms):\n",
        "        self.transforms = transforms\n",
        "\n",
        "    def __call__(self, image, mask):\n",
        "\n",
        "        for transform in self.transforms:\n",
        "            if torch.rand(1) < 0.5:\n",
        "                image = transform(image)\n",
        "                mask = transform(mask)\n",
        "        return image, mask\n",
        "\n",
        "custom_transform = RandomApplyTransforms([\n",
        "    TF.hflip,  # Horizontal flip\n",
        "    TF.vflip,  # Vertical flip\n",
        "\n",
        "])\n",
        "\n",
        "class TailsDatasetReconstruct(Dataset):\n",
        "    def __init__(self, raw_data_dir, image_transform=None):\n",
        "        self.raw_data_dir = raw_data_dir\n",
        "        self.image_transform = image_transform\n",
        "        self.filenames = [f.split('.')[0] for f in os.listdir(raw_data_dir) if f.endswith('.png')]\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.filenames)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        # Load images\n",
        "        raw_image_path = os.path.join(self.raw_data_dir, self.filenames[idx] + '.png')\n",
        "\n",
        "        raw_image = Image.open(raw_image_path).convert('RGB')\n",
        "\n",
        "        # Apply transformations\n",
        "        if self.image_transform is not None:\n",
        "            raw_image = self.image_transform(raw_image)\n",
        "\n",
        "        raw_image, raw_image = custom_transform(raw_image, raw_image)\n",
        "\n",
        "        return raw_image, raw_image\n",
        "\n",
        "    def set_transform(self, raw_transform):\n",
        "\n",
        "      self.image_transform = raw_transform\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Tiles dataset for model training (segmentation)"
      ],
      "metadata": {
        "id": "t6E24sGaQfa_"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jXF9RXyFekZJ"
      },
      "outputs": [],
      "source": [
        "from PIL import Image\n",
        "import os\n",
        "from torch.utils.data import Dataset\n",
        "from torchvision.transforms import functional as TF\n",
        "import torch\n",
        "from torchvision.transforms import v2\n",
        "\n",
        "class RandomApplyTransforms:\n",
        "    def __init__(self, transforms):\n",
        "        self.transforms = transforms\n",
        "\n",
        "    def __call__(self, image, mask):\n",
        "        # Apply each transformation to both image and mask\n",
        "        for transform in self.transforms:\n",
        "            if torch.rand(1) < 0.5:\n",
        "                image = transform(image)\n",
        "                mask = transform(mask)\n",
        "        return image, mask\n",
        "\n",
        "custom_transform = RandomApplyTransforms([\n",
        "    TF.hflip,  # Horizontal flip\n",
        "    TF.vflip,  # Vertical flip\n",
        "\n",
        "])\n",
        "\n",
        "class TailsDatasetMask(Dataset):\n",
        "    def __init__(self, raw_data_dir, masks_dir, image_transform=None, mask_transform=None):\n",
        "        self.raw_data_dir = raw_data_dir\n",
        "        self.masks_dir = masks_dir\n",
        "        self.image_transform = image_transform\n",
        "        self.mask_transform = mask_transform\n",
        "        self.filenames = [f.split('.')[0] for f in os.listdir(masks_dir) if f.endswith('.png')]\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.filenames)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        # Load images\n",
        "        raw_image_path = os.path.join(self.raw_data_dir, self.filenames[idx] + '.png')\n",
        "        mask_path = os.path.join(self.masks_dir, self.filenames[idx] + '.png')\n",
        "\n",
        "        raw_image = Image.open(raw_image_path).convert('RGB')\n",
        "        mask = Image.open(mask_path).convert('L')\n",
        "\n",
        "        # Apply transformations\n",
        "        if self.image_transform is not None:\n",
        "            raw_image = self.image_transform(raw_image)\n",
        "\n",
        "\n",
        "        if self.mask_transform is not None:\n",
        "            mask = self.mask_transform(mask)\n",
        "\n",
        "        # mask = self.reverse_labels_in_mask(mask)\n",
        "        raw_image, mask = custom_transform(raw_image, mask)\n",
        "\n",
        "        return raw_image,  mask\n",
        "\n",
        "    def set_transform(self, raw_transform, mask_transform):\n",
        "\n",
        "      self.image_transform = raw_transform\n",
        "      self.mask_transform = mask_transform\n",
        "\n",
        "    def reverse_labels_in_mask(self, mask):\n",
        "        mask = 1 - mask\n",
        "        return mask\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XdedYD6Ye7pl"
      },
      "outputs": [],
      "source": [
        "from torch.utils.data import DataLoader, random_split\n",
        "\n",
        "raw_transform = v2.Compose([\n",
        "    v2.ColorJitter(brightness=0.5, contrast = 0.5),\n",
        "    v2.ToImage(), v2.ToDtype(torch.float32, scale=True)\n",
        "])\n",
        "\n",
        "test_raw_transform = v2.Compose([v2.ToImage(), v2.ToDtype(torch.float32, scale=True)])\n",
        "\n",
        "mask_transform = v2.Compose([v2.ToImage(), v2.ToDtype(torch.float32, scale=True)])\n",
        "\n",
        "\n",
        "\n",
        "# Initialize your dataset with the combined transform\n",
        "dataset_m = TailsDatasetMask(raw_data_dir=BASE_PATH+'Tiles/raw',\n",
        "                                  masks_dir=BASE_PATH+'Tiles/mask',\n",
        "                                  image_transform=raw_transform, mask_transform=mask_transform)\n",
        "\n",
        "# Split the dataset into training, validation, and test sets\n",
        "train_size = int(0.9 * len(dataset_m))\n",
        "val_size = int(0.1 * len(dataset_m))\n",
        "test_size = len(dataset_m) - train_size - val_size\n",
        "train_dataset, val_dataset, test_dataset = random_split(dataset_m, [train_size, val_size, test_size])\n",
        "\n",
        "\n",
        "# Create DataLoader objects for each dataset\n",
        "train_loader_mask = DataLoader(train_dataset, batch_size=30, shuffle=True, num_workers =6)\n",
        "val_loader_mask = DataLoader(val_dataset, batch_size=10, shuffle=False)\n",
        "test_loader_mask = DataLoader(test_dataset, batch_size=15, shuffle=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7zvnKPfIy22j"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.utils.data import DataLoader, random_split\n",
        "from torchvision.transforms import v2\n",
        "\n",
        "\n",
        "\n",
        "# Function to calculate class weights\n",
        "def calculate_class_weights(loader, num_classes=2):\n",
        "    class_counts = torch.zeros(num_classes)\n",
        "\n",
        "    for _, masks in loader:\n",
        "        masks = masks.view(-1)\n",
        "        class_counts[0] += (masks == 0).sum().item()\n",
        "        class_counts[1] += (masks == 1).sum().item()\n",
        "\n",
        "    # Calculate weights as the inverse of class frequency\n",
        "    total_count = class_counts.sum()\n",
        "    # print(class_counts )\n",
        "    class_weights = class_weights = total_count / (num_classes * class_counts)\n",
        "    return class_weights\n",
        "\n",
        "# Calculate weights for the training data\n",
        "class_weights = calculate_class_weights(train_loader_mask)\n",
        "print(f'Class Weights: {class_weights}')\n",
        "\n",
        "# Define the loss function with class weights\n",
        "criterion_seg = nn.BCEWithLogitsLoss(pos_weight=class_weights[1])\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_UrPt5Oc1_Zq"
      },
      "outputs": [],
      "source": [
        "raw_transform_recon = v2.Compose([\n",
        "    v2.ColorJitter(brightness=0.5, contrast = 0.5),\n",
        "    v2.ToImage(), v2.ToDtype(torch.float32, scale=True)\n",
        "])\n",
        "\n",
        "# Initialize your dataset with the combined transform\n",
        "dataset_rec = TailsDatasetReconstruct(raw_data_dir=BASE_PATH+'Tiles/Semi-supervised/raw',\n",
        "                                  image_transform=raw_transform_recon)\n",
        "\n",
        "# Split the dataset into training, validation, and test sets\n",
        "train_size = int(0.9 * len(dataset_rec))\n",
        "val_size = int(0.1 * len(dataset_rec))\n",
        "\n",
        "train_dataset_rec, val_dataset_rec = random_split(dataset_rec, [train_size, val_size])\n",
        "\n",
        "# Create DataLoader objects for each dataset\n",
        "train_loader_rec = DataLoader(train_dataset_rec, batch_size=30, shuffle=True, num_workers =4)\n",
        "val_loader_rec = DataLoader(val_dataset_rec, batch_size=10, shuffle=False)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eWD4_Eq66i6y"
      },
      "source": [
        "# Models"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YBdDbZ2B6oLn"
      },
      "source": [
        "## Original Unet"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kFi6FHbv6zew"
      },
      "outputs": [],
      "source": [
        "\"\"\" Parts of the U-Net model \"\"\"\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "\n",
        "class DoubleConv(nn.Module):\n",
        "    \"\"\"(convolution => [BN] => ReLU) * 2\"\"\"\n",
        "\n",
        "    def __init__(self, in_channels, out_channels, mid_channels=None):\n",
        "        super().__init__()\n",
        "        if not mid_channels:\n",
        "            mid_channels = out_channels\n",
        "        self.double_conv = nn.Sequential(\n",
        "            nn.Conv2d(in_channels, mid_channels, kernel_size=3, padding=1),\n",
        "            nn.BatchNorm2d(mid_channels),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Conv2d(mid_channels, out_channels, kernel_size=3, padding=1),\n",
        "            nn.BatchNorm2d(out_channels),\n",
        "            nn.ReLU(inplace=True)\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.double_conv(x)\n",
        "\n",
        "\n",
        "class Down(nn.Module):\n",
        "    \"\"\"Downscaling with maxpool then double conv\"\"\"\n",
        "\n",
        "    def __init__(self, in_channels, out_channels):\n",
        "        super().__init__()\n",
        "        self.maxpool_conv = nn.Sequential(\n",
        "            nn.MaxPool2d(2),\n",
        "            DoubleConv(in_channels, out_channels)\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.maxpool_conv(x)\n",
        "\n",
        "\n",
        "class Up(nn.Module):\n",
        "    \"\"\"Upscaling then double conv\"\"\"\n",
        "\n",
        "    def __init__(self, in_channels, out_channels, bilinear=True):\n",
        "        super().__init__()\n",
        "\n",
        "        # if bilinear, use the normal convolutions to reduce the number of channels\n",
        "        if bilinear:\n",
        "            self.up = nn.Upsample(scale_factor=2, mode='bilinear', align_corners=True)\n",
        "            self.conv = DoubleConv(in_channels, out_channels, in_channels // 2)\n",
        "        else:\n",
        "            self.up = nn.ConvTranspose2d(in_channels, in_channels // 2, kernel_size=2, stride=2)\n",
        "            self.conv = DoubleConv(in_channels, out_channels)\n",
        "\n",
        "    def forward(self, x1, x2):\n",
        "        x1 = self.up(x1)\n",
        "        # input is CHW\n",
        "        diffY = x2.size()[2] - x1.size()[2]\n",
        "        diffX = x2.size()[3] - x1.size()[3]\n",
        "\n",
        "        x1 = F.pad(x1, [diffX // 2, diffX - diffX // 2,\n",
        "                        diffY // 2, diffY - diffY // 2])\n",
        "        # if you have padding issues, see\n",
        "        # https://github.com/HaiyongJiang/U-Net-Pytorch-Unstructured-Buggy/commit/0e854509c2cea854e247a9c615f175f76fbb2e3a\n",
        "        # https://github.com/xiaopeng-liao/Pytorch-UNet/commit/8ebac70e633bac59fc22bb5195e513d5832fb3bd\n",
        "        x = torch.cat([x2, x1], dim=1)\n",
        "        return self.conv(x)\n",
        "\n",
        "\n",
        "class OutConv(nn.Module):\n",
        "    def __init__(self, in_channels, out_channels):\n",
        "        super(OutConv, self).__init__()\n",
        "        self.conv = nn.Conv2d(in_channels, out_channels, kernel_size=1)\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.conv(x)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "khHLD8Iu65wv"
      },
      "outputs": [],
      "source": [
        "\"\"\" Full assembly of the parts to form the complete network \"\"\"\n",
        "\n",
        "# from unet_parts import *\n",
        "\n",
        "\n",
        "class UNet(nn.Module):\n",
        "    def __init__(self, n_channels, n_classes, bilinear=True):\n",
        "        super(UNet, self).__init__()\n",
        "        self.n_channels = n_channels\n",
        "        self.n_classes = n_classes\n",
        "        self.bilinear = bilinear\n",
        "\n",
        "        self.inc = DoubleConv(n_channels, 64)\n",
        "        self.down1 = Down(64, 128)\n",
        "        self.down2 = Down(128, 256)\n",
        "        self.down3 = Down(256, 512)\n",
        "        factor = 2 if bilinear else 1\n",
        "        self.down4 = Down(512, 1024 // factor)\n",
        "        self.up1 = Up(1024, 512 // factor, bilinear)\n",
        "        self.up2 = Up(512, 256 // factor, bilinear)\n",
        "        self.up3 = Up(256, 128 // factor, bilinear)\n",
        "        self.up4 = Up(128, 64, bilinear)\n",
        "        self.outc = OutConv(64, n_classes)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x1 = self.inc(x)\n",
        "        x2 = self.down1(x1)\n",
        "        x3 = self.down2(x2)\n",
        "        x4 = self.down3(x3)\n",
        "        x5 = self.down4(x4)\n",
        "        x = self.up1(x5, x4)\n",
        "        x = self.up2(x, x3)\n",
        "        x = self.up3(x, x2)\n",
        "        x = self.up4(x, x1)\n",
        "        logits = self.outc(x)\n",
        "        return logits"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TWjO3_XW66-t"
      },
      "source": [
        "## Modified Unet model with 2 decoders"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3ik9Maja7JPt"
      },
      "outputs": [],
      "source": [
        "\"\"\" Parts of the U-Net model \"\"\"\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "\n",
        "class DoubleConv(nn.Module):\n",
        "    \"\"\"(convolution => [BN] => ReLU) * 2\"\"\"\n",
        "\n",
        "    def __init__(self, in_channels, out_channels, mid_channels=None):\n",
        "        super().__init__()\n",
        "        if not mid_channels:\n",
        "            mid_channels = out_channels\n",
        "        self.double_conv = nn.Sequential(\n",
        "            nn.Conv2d(in_channels, mid_channels, kernel_size=3, padding=1),\n",
        "            nn.BatchNorm2d(mid_channels),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Conv2d(mid_channels, out_channels, kernel_size=3, padding=1),\n",
        "            nn.BatchNorm2d(out_channels),\n",
        "            nn.ReLU(inplace=True)\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.double_conv(x)\n",
        "\n",
        "\n",
        "class Down(nn.Module):\n",
        "    \"\"\"Downscaling with maxpool then double conv\"\"\"\n",
        "\n",
        "    def __init__(self, in_channels, out_channels):\n",
        "        super().__init__()\n",
        "        self.maxpool_conv = nn.Sequential(\n",
        "            nn.MaxPool2d(2),\n",
        "            DoubleConv(in_channels, out_channels)\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.maxpool_conv(x)\n",
        "\n",
        "\n",
        "class Up(nn.Module):\n",
        "\n",
        "    \"\"\"Modified Upscaling then double conv\"\"\"\n",
        "    def __init__(self, in_channels, out_channels, bilinear=True, use_skip=True):\n",
        "        super().__init__()\n",
        "        self.use_skip = use_skip\n",
        "        if bilinear:\n",
        "            self.up = nn.Upsample(scale_factor=2, mode='bilinear', align_corners=True)\n",
        "            self.conv = DoubleConv(in_channels, out_channels, in_channels // 2)\n",
        "        else:\n",
        "            self.up = nn.ConvTranspose2d(in_channels, in_channels // 2, kernel_size=2, stride=2) #check this part first\n",
        "        if use_skip:\n",
        "            self.conv = DoubleConv(in_channels, out_channels, in_channels // 2)\n",
        "        else:\n",
        "            self.conv = DoubleConv(out_channels, out_channels, in_channels // 2)\n",
        "\n",
        "    def forward(self, x1, x2):\n",
        "        x1 = self.up(x1)\n",
        "        # print(f'x1 up: {x1.shape}')\n",
        "        if self.use_skip and x2 is not None:\n",
        "            diffY = x2.size()[2] - x1.size()[2]\n",
        "            diffX = x2.size()[3] - x1.size()[3]\n",
        "            x1 = F.pad(x1, [diffX // 2, diffX - diffX // 2, diffY // 2, diffY - diffY // 2])\n",
        "            x = torch.cat([x2, x1], dim=1)\n",
        "            # print('skip')\n",
        "        else:\n",
        "            x = x1  # Without skip connection\n",
        "            # print(f'x1 up: {x.shape}')\n",
        "        x = self.conv(x)\n",
        "        # print(f'x up: {x.shape}')\n",
        "        return x\n",
        "\n",
        "\n",
        "class OutConv(nn.Module):\n",
        "    def __init__(self, in_channels, out_channels):\n",
        "        super(OutConv, self).__init__()\n",
        "        self.conv = nn.Conv2d(in_channels, out_channels, kernel_size=1)\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.conv(x)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mw_h_Ok66mBu"
      },
      "outputs": [],
      "source": [
        "class UNetWithDualDecoders(nn.Module):\n",
        "    def __init__(self, n_channels, n_classes, bilinear=False):\n",
        "        super(UNetWithDualDecoders, self).__init__()\n",
        "        self.n_channels = n_channels\n",
        "        self.n_classes = n_classes\n",
        "        self.bilinear = bilinear\n",
        "\n",
        "        # Shared Encoder\n",
        "        self.inc = DoubleConv(n_channels, 64)\n",
        "        self.down1 = Down(64, 128)\n",
        "        self.down2 = Down(128, 256)\n",
        "        self.down3 = Down(256, 512)\n",
        "        factor = 2 if bilinear else 1\n",
        "        self.down4 = Down(512, 1024 // factor)\n",
        "\n",
        "        # Decoder for Segmentation\n",
        "        self.up1 = Up(1024, 512 // factor, bilinear)\n",
        "        self.up2 = Up(512, 256 // factor, bilinear)\n",
        "        self.up3 = Up(256, 128 // factor, bilinear)\n",
        "        self.up4 = Up(128, 64, bilinear)\n",
        "        self.outc_seg = OutConv(64, n_classes)\n",
        "\n",
        "        # Decoder for Reconstruction\n",
        "        self.recon_up1 = Up(1024, 512 // factor, bilinear, use_skip=False)\n",
        "        # print(f'x_recon1: {self.recon_up1.shape}')\n",
        "        self.recon_up2 = Up(512, 256 // factor, bilinear, use_skip=False)\n",
        "        self.recon_up3 = Up(256, 128 // factor, bilinear, use_skip=False)\n",
        "        self.recon_up4 = Up(128, 64, bilinear, use_skip=False)\n",
        "        self.outc_recon = OutConv(64, n_channels)\n",
        "\n",
        "    def forward(self, x, train_recon=False, freeze_encoder=False):\n",
        "        # Encoder\n",
        "        x1 = self.inc(x)\n",
        "        # print(f'x1: {x1.shape}')\n",
        "        x2 = self.down1(x1)\n",
        "        # print(f'x2: {x2.shape}')\n",
        "        x3 = self.down2(x2)\n",
        "        # print(f'x3: {x3.shape}')\n",
        "        x4 = self.down3(x3)\n",
        "        # print(f'x4: {x4.shape}')\n",
        "        x5 = self.down4(x4)\n",
        "        # print(f'x5: {x5.shape}')\n",
        "\n",
        "\n",
        "        if train_recon:\n",
        "            # Decoder for Reconstruction\n",
        "            x_recon = self.recon_up1(x5, None)  # No skipped connection\n",
        "            # print(f'x_recon1: {x_recon.shape}')\n",
        "            x_recon = self.recon_up2(x_recon, None)\n",
        "            # print(f'x_recon2: {x_recon.shape}')\n",
        "            x_recon = self.recon_up3(x_recon, None)\n",
        "            # print(f'x_recon3: {x_recon.shape}')\n",
        "            x_recon = self.recon_up4(x_recon, None)\n",
        "            # print(f'x_recon4: {x_recon.shape}')\n",
        "            recon = self.outc_recon(x_recon)\n",
        "            recon = nn.Sigmoid()(recon)  # Use Sigmoid for activation to ensure non-negative output\n",
        "            # print(f'recon: {recon.shape}')\n",
        "            return recon\n",
        "        else:\n",
        "            # Decoder for Segmentation\n",
        "            x_seg = self.up1(x5, x4)\n",
        "            # print(f'x_seg1: {x_seg.shape}')\n",
        "            x_seg = self.up2(x_seg, x3)\n",
        "            # print(f'x_seg2: {x_seg.shape}')\n",
        "            x_seg = self.up3(x_seg, x2)\n",
        "            # print(f'x_seg3: {x_seg.shape}')\n",
        "            x_seg = self.up4(x_seg, x1)\n",
        "            # print(f'x_seg4: {x_seg.shape}')\n",
        "            logits_seg = self.outc_seg(x_seg)\n",
        "            #logits_seg = nn.Sigmoid()(logits_seg)\n",
        "            # print(f'logits_seg: {logits_seg.shape}')\n",
        "            return logits_seg\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Z4XkY9XwyP7i"
      },
      "source": [
        "# Model setup"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YxT807iOyhZc"
      },
      "outputs": [],
      "source": [
        "# print(class_weights)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GgnRaYV-4dvD"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.utils.data import DataLoader\n",
        "from torchvision import datasets, models, transforms\n",
        "from torchvision.models.segmentation import deeplabv3_resnet50, fcn_resnet50, deeplabv3_mobilenet_v3_large, deeplabv3_resnet101, FCN_ResNet50_Weights, DeepLabV3_ResNet50_Weights\n",
        "import torch.optim as optim\n",
        "from torch.optim.lr_scheduler import StepLR\n",
        "from torchvision.transforms import functional as TF\n",
        "import os\n",
        "from PIL import Image\n",
        "import numpy as np"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nzOsbzs_04Vc"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "\n",
        "# Define loss functions\n",
        "# Define the loss function with class weights\n",
        "# criterion_seg = nn.BCEWithLogitsLoss()\n",
        "criterion_seg = nn.BCEWithLogitsLoss(pos_weight=class_weights[1])\n",
        "criterion_recon = nn.L1Loss()\n",
        "\n",
        "# Assuming you have DataLoader instances: train_loader and test_loader\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "model = UNetWithDualDecoders(n_channels=3, n_classes=1).to(device)\n",
        "\n",
        "\n",
        "# Assume optimizer is defined for your model parameters\n",
        "optimizer = optim.Adam(model.parameters(), lr=0.0001)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Train"
      ],
      "metadata": {
        "id": "UUdjwt9mUMJE"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "miByVvWh0-Gg"
      },
      "outputs": [],
      "source": [
        "from tqdm import tqdm\n",
        "import torch\n",
        "from tqdm.auto import tqdm\n",
        "\n",
        "def train_combined(model, train_loader_mask, train_loader_rec, optimizer, device, lambda_recon=0.5):\n",
        "    model.train()\n",
        "    running_loss = 0.0\n",
        "\n",
        "    # Create iterators for both DataLoaders\n",
        "    iter_mask = iter(train_loader_mask)\n",
        "    iter_rec = iter(train_loader_rec)\n",
        "\n",
        "    # Calculate the total number of batches. Assuming train_loader_rec is the larger dataset.\n",
        "    total_batches = len(train_loader_rec)\n",
        "\n",
        "    pbar = tqdm(total=total_batches, leave=True)\n",
        "\n",
        "    for _ in range(total_batches):\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        # Handle mask generation task if mask data is available\n",
        "        try:\n",
        "            data_mask, target_mask = next(iter_mask)\n",
        "            data_mask, target_mask = data_mask.to(device), target_mask.to(device)\n",
        "            output_mask = model(data_mask, train_recon=False, freeze_encoder=False)\n",
        "            loss_mask = criterion_seg(output_mask, target_mask)\n",
        "        except StopIteration:\n",
        "            # No more mask data; mask dataset is smaller\n",
        "            loss_mask = 0\n",
        "\n",
        "        # Handle reconstruction task\n",
        "        try:\n",
        "            data_rec, _ = next(iter_rec)  # Assuming reconstruction targets are the inputs themselves\n",
        "            data_rec = data_rec.to(device)\n",
        "            output_rec = model(data_rec, train_recon=True, freeze_encoder=False)\n",
        "            loss_rec = criterion_recon(output_rec, data_rec)\n",
        "        except StopIteration:\n",
        "            # Just in case reconstruction dataset finishes early, which shouldn't happen in this setup\n",
        "            loss_rec = 0\n",
        "\n",
        "        # Combine losses, with an option to weight the reconstruction loss\n",
        "        loss = loss_mask + lambda_recon * loss_rec\n",
        "\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        running_loss += loss.item()\n",
        "        pbar.update(1)\n",
        "        pbar.set_description(f'Epoch Loss: {running_loss/(pbar.n+1):.4f}')\n",
        "\n",
        "    pbar.close()\n",
        "    print(f'Training Loss: {running_loss / total_batches:.4f}')\n",
        "    return running_loss / total_batches\n",
        "\n",
        "def train(model, dataloader, optimizer, device, train_recon=False, freeze_encoder=False):\n",
        "    model.train()\n",
        "    running_loss = 0.0\n",
        "    pbar = tqdm(enumerate(dataloader), total=len(dataloader), leave=False)\n",
        "    for batch_idx, (data, target) in pbar:\n",
        "        data = data.to(device)\n",
        "        target = target.to(device)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        if train_recon:\n",
        "            output = model(data, train_recon=True, freeze_encoder=freeze_encoder)\n",
        "            loss = criterion_recon(output, target)\n",
        "        else:\n",
        "            output = model(data, train_recon=False, freeze_encoder=freeze_encoder)\n",
        "            loss = criterion_seg(output, target)\n",
        "\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        running_loss += loss.item()\n",
        "        pbar.set_description(f'Epoch Loss: {running_loss/(batch_idx+1):.4f}')\n",
        "\n",
        "    print(f'Training Loss: {running_loss / len(dataloader):.4f}')\n",
        "    return running_loss / len(dataloader)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Test"
      ],
      "metadata": {
        "id": "ETE0UfhvUQDE"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xS0abNhf1MMZ"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "\n",
        "def accuracy(output, target):\n",
        "    preds = output.round()  # Round predictions to 0 or 1\n",
        "    correct = (preds == target).float()  # Element-wise equality\n",
        "    acc = correct.sum() / correct.numel()\n",
        "    return acc\n",
        "\n",
        "def test(model, dataloader, device, test_recon=False, num_visualizations=3):\n",
        "    model.eval()\n",
        "    running_loss = 0.0\n",
        "    total_accuracy = 0.0 if not test_recon else None\n",
        "    visualized = 0\n",
        "    with torch.no_grad(), tqdm(enumerate(dataloader), total=len(dataloader), leave=False) as pbar:\n",
        "        for batch_idx, (data, target) in pbar:\n",
        "            data = data.to(device)\n",
        "            if test_recon:\n",
        "                target = data\n",
        "            else:\n",
        "                target = target.to(device)\n",
        "\n",
        "            output = model(data, train_recon=test_recon, freeze_encoder=False)\n",
        "            # output = model(data)['out']\n",
        "\n",
        "            loss = criterion_recon(output, target) if test_recon else criterion_seg(output, target)\n",
        "            running_loss += loss.item()\n",
        "\n",
        "            if not test_recon:\n",
        "                acc = accuracy(output, target)\n",
        "                total_accuracy += acc.item()\n",
        "\n",
        "            # Visualization for the first N samples\n",
        "            if visualized < num_visualizations:\n",
        "                if test_recon:\n",
        "                    fig, ax = plt.subplots(1, 2, figsize=(12, 4))\n",
        "                    ax[0].imshow(data[0].cpu().permute(1, 2, 0))\n",
        "                    ax[0].set_title(\"Input Image\")\n",
        "                    ax[1].imshow(output[0].cpu().permute(1, 2, 0))\n",
        "                    ax[1].set_title(\"Reconstructed Image\")\n",
        "                    # ax[2].imshow(target[0].cpu().numpy().transpose(1, 2, 0))\n",
        "                    # ax[2].set_title(\"Target Image\")\n",
        "                    plt.show()\n",
        "                else:\n",
        "                    preds = torch.sigmoid(output[0].cpu().squeeze()) > 0.9\n",
        "                    # preds = output[0].cpu().squeeze()\n",
        "                    # Assuming segmentation masks are single-channel\n",
        "                    fig, ax = plt.subplots(1, 2, figsize=(12, 4))\n",
        "                    ax[0].imshow(data[0].cpu().numpy().transpose(1, 2, 0))\n",
        "                    ax[0].set_title(\"Input Image\")\n",
        "                    ax[1].imshow(preds, cmap='gray')\n",
        "                    ax[1].set_title(\"Predicted Mask\")\n",
        "                    # print(output[0].cpu().squeeze())\n",
        "                    # ax[2].imshow(target[0].cpu(), cmap='gray')\n",
        "                    # ax[2].set_title(\"True Mask\")\n",
        "                    # print(target[0].cpu().squeeze())\n",
        "                    plt.show()\n",
        "                visualized += 1\n",
        "\n",
        "\n",
        "            pbar.set_description(f'Batch {batch_idx+1}, Loss: {loss.item():.4f}')\n",
        "\n",
        "    epoch_loss = running_loss / len(dataloader)\n",
        "    print(f'Test Loss: {epoch_loss:.4f}')\n",
        "    if not test_recon:\n",
        "        epoch_accuracy = total_accuracy / len(dataloader)\n",
        "        print(f'Test Accuracy: {epoch_accuracy:.4f}')\n",
        "\n",
        "    return epoch_loss\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1fQscIdARUlN"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Validate"
      ],
      "metadata": {
        "id": "ScdoMDEbUVGW"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dbtspfTJTBtf"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import DataLoader\n",
        "from torchvision import transforms\n",
        "from tqdm import tqdm\n",
        "import torch\n",
        "from skimage.morphology import skeletonize\n",
        "\n",
        "def calculate_accuracy(output, target):\n",
        "    # Assuming output is logits, threshold to get binary prediction\n",
        "    preds = torch.sigmoid(output) > 0.9\n",
        "\n",
        "    correct = (preds == target).float()  # Convert to float for division\n",
        "\n",
        "    accuracy = correct.sum() / correct.numel()\n",
        "    return accuracy\n",
        "\n",
        "def calculate_precision_recall_f1(output, target, epsilon=1e-7):\n",
        "    preds = torch.sigmoid(output) > 0.9\n",
        "    true_positives = (preds * target).sum()\n",
        "    predicted_positives = preds.sum()\n",
        "    actual_positives = target.sum()\n",
        "\n",
        "    precision = true_positives / (predicted_positives + epsilon)\n",
        "    recall = true_positives / (actual_positives + epsilon)\n",
        "    f1 = 2 * (precision * recall) / (precision + recall + epsilon)\n",
        "    return precision, recall, f1\n",
        "\n",
        "def calculate_iou(output, target, epsilon=1e-7):\n",
        "    preds = torch.sigmoid(output) > 0.9\n",
        "    # skeleton = skeletonize(preds)\n",
        "    intersection = (preds * target).sum()\n",
        "    union = preds.sum() + target.sum() - intersection\n",
        "    iou = intersection / (union + epsilon)\n",
        "    return iou"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hCTwrEY0Rd5A"
      },
      "outputs": [],
      "source": [
        "def validation(model, device, val_loader, criterion):\n",
        "    model.eval()\n",
        "    val_loss = 0.0\n",
        "    total_accuracy = 0.0\n",
        "    total_precision = 0.0\n",
        "    total_recall = 0.0\n",
        "    total_f1 = 0.0\n",
        "    total_iou = 0.0\n",
        "    with torch.no_grad():\n",
        "        for data, target in val_loader:\n",
        "            data, target = data.to(device), target.to(device)\n",
        "\n",
        "            output = model(data)\n",
        "            # output = model(data)['out']\n",
        "\n",
        "            val_loss += criterion(output, target).item()\n",
        "\n",
        "            accuracy = calculate_accuracy(output, target)\n",
        "            precision, recall, f1 = calculate_precision_recall_f1(output, target)\n",
        "            iou = calculate_iou(output, target)\n",
        "\n",
        "            total_accuracy += accuracy.item()\n",
        "            total_precision += precision.item()\n",
        "            total_recall += recall.item()\n",
        "            total_f1 += f1.item()\n",
        "            total_iou += iou.item()\n",
        "\n",
        "    # Calculate averages\n",
        "    num_batches = len(val_loader)\n",
        "    avg_loss = val_loss / num_batches\n",
        "    avg_accuracy = total_accuracy / num_batches\n",
        "    avg_precision = total_precision / num_batches\n",
        "    avg_recall = total_recall / num_batches\n",
        "    avg_f1 = total_f1 / num_batches\n",
        "    avg_iou = total_iou / num_batches\n",
        "\n",
        "\n",
        "    avg_metrics = {\n",
        "    \"avg_val_loss\": avg_loss,\n",
        "    \"avg_val_accuracy\": avg_accuracy,\n",
        "    \"avg_val_precision\": avg_precision,\n",
        "    \"avg_val_recall\": avg_recall,\n",
        "    \"avg_val_f1\": avg_f1,\n",
        "    \"avg_val_iou\": avg_iou,\n",
        "    }\n",
        "\n",
        "    # wandb.log(avg_metrics)\n",
        "    print(f'Validation Loss: {avg_loss:.4f}, Accuracy: {avg_accuracy:.4f}, Precision: {avg_precision:.4f}, Recall: {avg_recall:.4f}, F1: {avg_f1:.4f}, IoU: {avg_iou:.4f}')\n",
        "\n",
        "    return avg_loss, avg_accuracy, avg_iou, avg_f1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2Cu8uDGg_Pxi"
      },
      "outputs": [],
      "source": [
        "len(train_loader_rec)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DSP-GX5KECNt"
      },
      "outputs": [],
      "source": [
        "len(val_loader_rec)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5UDPHOU4H4AS"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jwSQTd_tU8GD"
      },
      "source": [
        "# Train the model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jcqQV-BeRMJL"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import gc\n",
        "\n",
        "def cleanup_gpu():\n",
        "    gc.collect()\n",
        "    torch.cuda.empty_cache()\n",
        "\n",
        "cleanup_gpu()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QHzevFUPl4w5"
      },
      "outputs": [],
      "source": [
        "from IPython.display import clear_output\n",
        "# Function to evaluate and visualize predicted images\n",
        "def visualize_reconstruction(model, data_loader, num_images=5):\n",
        "    model.eval()\n",
        "    with torch.no_grad():\n",
        "        data_iter = iter(data_loader)\n",
        "        images, _ = next(data_iter)\n",
        "        images = images.to(device)\n",
        "        outputs = model(images)\n",
        "\n",
        "        images = images.cpu().numpy()\n",
        "        outputs = outputs.cpu().numpy()\n",
        "\n",
        "        # Rescale images\n",
        "        images = images\n",
        "        outputs = outputs\n",
        "\n",
        "        plt.figure(figsize=(20, 8))\n",
        "        for i in range(num_images):\n",
        "            # Original images\n",
        "            ax = plt.subplot(2, num_images, i + 1)\n",
        "            plt.imshow(images[i].transpose(1, 2, 0).squeeze(), cmap='gray')\n",
        "            plt.title(\"Original\")\n",
        "            plt.axis('off')\n",
        "\n",
        "            # Reconstructed images\n",
        "            ax = plt.subplot(2, num_images, i + 1 + num_images)\n",
        "            plt.imshow(outputs[i].transpose(1, 2, 0).squeeze(), cmap='gray')\n",
        "            plt.title(\"Reconstructed\")\n",
        "            plt.axis('off')\n",
        "\n",
        "        plt.show()\n",
        "\n",
        "# Function to plot the training and validation losses in real-time\n",
        "def plot_losses(epoch_losses, val_losses):\n",
        "    clear_output(wait=True)\n",
        "    plt.figure(figsize=(10, 5))\n",
        "    plt.plot(epoch_losses, label='Training Loss')\n",
        "    plt.plot(val_losses, label='Validation Loss')\n",
        "    plt.xlabel('Epochs')\n",
        "    plt.ylabel('Loss')\n",
        "    plt.title('Training and Validation Loss over Epochs')\n",
        "    plt.legend()\n",
        "    plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4-zDU0AEnzNv"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "\n",
        "\n",
        "# Initialize early stopping parameters\n",
        "patience = 5  # Number of epochs to wait for improvement before stopping\n",
        "best_val_loss = float('inf')\n",
        "patience_counter = 0\n",
        "train_losess = []\n",
        "val_losess = []\n",
        "\n",
        "num_epochs = 100  # Adjust as needed\n",
        "for epoch in range(1, num_epochs + 1):\n",
        "    print(f\"Epoch {epoch}/{num_epochs}\")\n",
        "\n",
        "    train(model, train_loader_rec, optimizer, device, train_recon=True, freeze_encoder=False)\n",
        "\n",
        "    # train_loss=train(model, train_loader_mask, optimizer, device, train_recon=False, freeze_encoder=False)\n",
        "    #train_loss = train_combined(model, train_loader_mask, train_loader_rec, optimizer, device, lambda_recon=1)\n",
        "\n",
        "    val_loss, avg_accuracy, avg_iou, avg_f1 = validation(model, device, val_loader_mask, criterion_seg)\n",
        "\n",
        "    #val_loss = test(model, val_loader_rec, device, test_recon=True)\n",
        "    print(val_loss)\n",
        "    test(model, val_loader_mask, device, test_recon=False)\n",
        "\n",
        "    train_losess.append(train_loss)\n",
        "    val_losess.append(val_loss)\n",
        "    plot_losses(train_losess, val_losess)\n",
        "\n",
        "    # Check for improvement\n",
        "    if val_loss < best_val_loss:\n",
        "        best_val_loss = val_loss\n",
        "        patience_counter = 0  # Reset patience counter if improvement is seen\n",
        "        # Save the best model\n",
        "        torch.save(model.state_dict(), 'best_model.pt')\n",
        "    else:\n",
        "        patience_counter += 1  # Increment patience counter if no improvement\n",
        "        print(patience_counter)\n",
        "\n",
        "    print(f'Validation Loss: {val_loss:.4f}, Accuracy: {avg_accuracy:.4f}, IoU: {avg_iou:.4f}, F1: {avg_f1:.4f}')\n",
        "\n",
        "    # Check if patience is exceeded\n",
        "    if patience_counter >= patience:\n",
        "        print(f'Early stopping triggered. No improvement in validation loss for {patience} consecutive epochs.')\n",
        "        break\n",
        "\n",
        "# Load the best model after training\n",
        "model.load_state_dict(torch.load('best_model.pt'))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cUFnMWfSklUF"
      },
      "outputs": [],
      "source": [
        "print(val_loss)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PIeBl3vokymN"
      },
      "outputs": [],
      "source": [
        "num_epochs = 50 # Adjust as needed\n",
        "for epoch in range(1, num_epochs + 1):\n",
        "    train(model, train_loader_mask, optimizer, device, train_recon=False, freeze_encoder=False)\n",
        "    # train_combined(model, train_loader_mask, train_loader_rec, optimizer, device, lambda_recon=0.5)\n",
        "    validation(model, device, val_loader_mask, criterion_seg)\n",
        "    # test(model, val_loader_rec, device, test_recon=True)\n",
        "    test(model, val_loader_mask, device, test_recon=False)\n",
        "    # For training reconstruction\n",
        "    print(epoch)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uSgZ_9IA1MDi"
      },
      "outputs": [],
      "source": [
        "\n",
        "\n",
        "num_epochs = 100 # Adjust as needed\n",
        "for epoch in range(1, num_epochs + 1):\n",
        "\n",
        "    # For training reconstruction\n",
        "    print(epoch)\n",
        "    train(model, train_loader_rec, optimizer, device, train_recon=True, freeze_encoder=False)\n",
        "    # # For testing reconstruction\n",
        "    test(model, val_loader_rec, device, test_recon=True)\n",
        "\n",
        "    # # For training segmentation\n",
        "    # train(model, train_loader_mask, optimizer, device, train_recon=False, freeze_encoder=False)\n",
        "\n",
        "    # # For testing segmentation\n",
        "    # test(model, val_loader_mask, device, test_recon=False)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Evaluate the model"
      ],
      "metadata": {
        "id": "Bl3LcpRNS1Tf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Load the best model after training\n",
        "model.load_state_dict(torch.load('best_model.pt'))"
      ],
      "metadata": {
        "id": "BGwS1kaAUiw-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NdOVhSbgD8ZV"
      },
      "outputs": [],
      "source": [
        "validation(model, device, val_loader_mask, criterion_seg)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "j849YqPVsdQJ"
      },
      "outputs": [],
      "source": [
        "# For testing segmentation\n",
        "test(model, val_loader_mask, device, test_recon=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IDGMl5KRuf-O"
      },
      "outputs": [],
      "source": [
        "# For testing recon\n",
        "test(model, val_loader_rec, device, test_recon=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Save the model"
      ],
      "metadata": {
        "id": "0nstxrl1SHb2"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "k6RrsLJjQgcX"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "\n",
        "def save_checkpoint(model, optimizer, epoch, filename):\n",
        "    checkpoint = {\n",
        "        'epoch': epoch,\n",
        "        'model_state_dict': model.state_dict(),\n",
        "        'optimizer_state_dict': optimizer.state_dict(),\n",
        "    }\n",
        "    torch.save(checkpoint, BASE_PATH+\"/models/\"+filename)\n",
        "    print(f\"Checkpoint saved to {filename}\")\n",
        "\n",
        "\n",
        "def load_checkpoint(model, optimizer, filename):\n",
        "    checkpoint = torch.load(BASE_PATH+\"/models/\"+filename)\n",
        "    model.load_state_dict(checkpoint['model_state_dict'])\n",
        "    optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
        "    epoch = checkpoint['epoch']\n",
        "    print(f\"Model and optimizer loaded from checkpoint at epoch {epoch}\")\n",
        "    return epoch"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DqAdMxee34np"
      },
      "source": [
        "# Predict the full image"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XTy6XfgWr-V_"
      },
      "outputs": [],
      "source": [
        "\n",
        "import torch\n",
        "import numpy as np\n",
        "from PIL import Image\n",
        "from torchvision.transforms import functional as TF\n",
        "from skimage.morphology import skeletonize\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "def predict_mask_full_image(model, image, tile_size=256, overlap=64, device='cuda'):\n",
        "    model.eval()#\n",
        "\n",
        "    # Calculate necessary padding to make the image divisible into tiles\n",
        "    width, height = image.size\n",
        "    pad_height = (tile_size - (height % (tile_size - overlap))) % (tile_size - overlap)\n",
        "    pad_width = (tile_size - (width % (tile_size - overlap))) % (tile_size - overlap)\n",
        "    print(f\"Original image size: {width}x{height}, Padding: {pad_width}x{pad_height}\")\n",
        "\n",
        "    # Pad image\n",
        "    padded_image = TF.pad(image, padding=(0, 0, pad_width, pad_height), padding_mode='reflect')\n",
        "    print(f\"Padded image size: {padded_image.width}x{padded_image.height}\")\n",
        "\n",
        "    full_mask = np.zeros((padded_image.height, padded_image.width))\n",
        "    count_map = np.zeros((padded_image.height, padded_image.width))\n",
        "\n",
        "\n",
        "    plt.imshow(padded_image)\n",
        "    plt.title(\"Padded Image\")\n",
        "    plt.show()\n",
        "\n",
        "    tile_count = 0\n",
        "\n",
        "    # Generate and process tiles\n",
        "    for y in range(0, padded_image.height - overlap, tile_size - overlap):\n",
        "        for x in range(0, padded_image.width - overlap, tile_size - overlap):\n",
        "            tile_count += 1\n",
        "\n",
        "            tile = padded_image.crop((x, y, min(x + tile_size, padded_image.width), min(y + tile_size, padded_image.height)))\n",
        "            tile_padded = TF.pad(tile, padding=(0, 0, tile_size - tile.width, tile_size - tile.height))  # Pad tile to ensure 256x256\n",
        "\n",
        "            # Process tile\n",
        "            tile_padded_tensor = TF.to_tensor(tile_padded).unsqueeze(0).to(device)  # Convert to tensor and add batch dimension\n",
        "\n",
        "            # Predict mask for tile\n",
        "            with torch.no_grad():\n",
        "                tile_mask_tensor = model(tile_padded_tensor)[\"out\"].squeeze().cpu()\n",
        "                tile_mask = torch.sigmoid(tile_mask_tensor).numpy() > 0.9\n",
        "                # tile_mask = skeletonize(tile_mask)\n",
        "                tile_mask = 1 - tile_mask\n",
        "            # Resize mask back to original tile size if padding was added\n",
        "            tile_mask_resized = tile_mask[:tile.height, :tile.width]\n",
        "\n",
        "\n",
        "\n",
        "            # Update full mask and count map\n",
        "            full_mask[y:y + tile.height, x:x + tile.width] += tile_mask_resized\n",
        "            count_map[y:y + tile.height, x:x + tile.width] += 1\n",
        "\n",
        "    # Average the overlaps\n",
        "    full_mask /= count_map\n",
        "\n",
        "    # Crop out any extra padding added to the image\n",
        "    final_mask = full_mask[:height, :width]\n",
        "\n",
        "\n",
        "    plt.imshow(final_mask, cmap='gray')\n",
        "    plt.title(\"Final Mask\")\n",
        "    plt.savefig(\"final.png\")\n",
        "    plt.show()\n",
        "\n",
        "    print(f\"Processed {tile_count} tiles.\")\n",
        "    return final_mask\n",
        "\n",
        "raw_data_dir= BASE_PATH+'Raw_data'\n",
        "filenames = [f.split('.')[0] for f in os.listdir(raw_data_dir) if f.endswith('.tif')]\n",
        "raw_image_path = os.path.join(raw_data_dir, filenames[60] + '.tif')\n",
        "raw_image = Image.open(raw_image_path).convert('RGB')\n",
        "\n",
        "final_mask = predict_mask_full_image(model, raw_image, device='cuda')\n",
        "Image.fromarray((final_mask * 255).astype(np.uint8)).save(raw_image.filename.split('.')[0] + '_mask.png')\n",
        "print(final_mask.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "X86J5lJcQsDl"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-JFOcfVeUMsL"
      },
      "outputs": [],
      "source": [
        "from PIL import Image, ImageEnhance, ImageChops\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "def overlay_mask_on_image(raw_image, mask):\n",
        "\n",
        "    if not isinstance(raw_image, Image.Image):\n",
        "        raise ValueError(\"raw_image must be a PIL.Image.Image object\")\n",
        "    if not isinstance(mask, Image.Image):\n",
        "        raise ValueError(\"mask must be a PIL.Image.Image object\")\n",
        "\n",
        "    # Resize mask to match the raw image size if necessary\n",
        "    if raw_image.size != mask.size:\n",
        "        mask = mask.resize(raw_image.size, Image.BILINEAR)\n",
        "\n",
        "    # Convert the mask to 'L' mode if it's not already\n",
        "    single_channel_image = mask.convert('L')\n",
        "    # mask = ImageEnhance.Contrast(mask).enhance(2.0)\n",
        "    #high_contrast_mask = Image.fromarray(((np_mask)*255).astype(np.uint8)).convert('L')\n",
        "    single_channel_image.save(\"high_contrast_mask.png\")\n",
        "\n",
        "\n",
        "    # Create an RGBA version of the single-channel image with some transparency\n",
        "    alpha = 10 # Adjust the alpha value to control transparency\n",
        "    single_channel_rgba = Image.merge('RGBA', (single_channel_image, single_channel_image, single_channel_image, Image.new('L', single_channel_image.size, alpha)))\n",
        "\n",
        "    # Composite the single-channel image onto the RGB image\n",
        "    composite_image = Image.alpha_composite(raw_image.convert('RGBA'), single_channel_rgba)\n",
        "\n",
        "\n",
        "    return composite_image\n",
        "\n",
        "Image.fromarray((final_mask * 255).astype(np.uint8)).save(\"final.png\")\n",
        "overlay_image = overlay_mask_on_image(raw_image, Image.fromarray((final_mask * 255).astype(np.uint8)))\n",
        "\n",
        "# Display the result\n",
        "plt.figure(figsize=(30, 30))\n",
        "plt.imshow(overlay_image)\n",
        "plt.axis('off')\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "G5i7Ry6p4tMH"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import cv2\n",
        "import numpy as np\n",
        "from skimage.morphology import skeletonize\n",
        "\n",
        "# Load the uploaded image\n",
        "image_path = 'high_contrast_mask-2.png'\n",
        "image = cv2.imread(image_path, cv2.IMREAD_GRAYSCALE)\n",
        "\n",
        "# Apply skeletonization using skimage\n",
        "binary_image = image > 200\n",
        "skeleton = skeletonize(binary_image)\n",
        "\n",
        "# Save the skeletonized image\n",
        "skeletonized_image_path = 'skeletonized_image.png'\n",
        "plt.imsave(skeletonized_image_path, skeleton, cmap='gray')\n",
        "\n",
        "# Display the original and skeletonized images\n",
        "fig, ax = plt.subplots(1, 2, figsize=(12, 6))\n",
        "ax[0].imshow(image, cmap='gray')\n",
        "ax[0].set_title('Original Image')\n",
        "ax[0].axis('off')\n",
        "\n",
        "ax[1].imshow(skeleton, cmap='gray')\n",
        "ax[1].set_title('Skeletonized Image')\n",
        "ax[1].axis('off')\n",
        "\n",
        "plt.show()\n",
        "\n",
        "skeletonized_image_path\n"
      ]
    }
  ]
}